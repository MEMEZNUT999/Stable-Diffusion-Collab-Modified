{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MEMEZNUT999/Stable-Diffusion-Collab-Modified/blob/main/Stable_Diffusion_WebUi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xox-Rn_-MTD"
      },
      "source": [
        "# Stable Diffusion Web UI Colab\n",
        "\n",
        "A browser interface based on Gradio library for Stable Diffusion running on Google Colab.\n",
        "For more detailed info please visit [this](https://en.wikipedia.org/wiki/Stable_Diffusion).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SlavyanDesu/sd-webui-colab/master/assets/image.png\" width=\"100%\">\n",
        "\n",
        "We provide a trained models (*SD v1.5, ChilloutMix, Grapefruit v4.1, Counterfeit v2.5, and etc.*) with pre-installed extensions.\n",
        "\n",
        "This colab is based on [camenduru/stable-diffusion-webui-colab](https://github.com/camenduru/stable-diffusion-webui-colab) and [TheLastBen/fast-stable-diffusion](https://github.com/TheLastBen/fast-stable-diffusion).\n",
        "\n",
        "Github: [SlavyanDesu/sd-webui-colab](https://github.com/SlavyanDesu/sd-webui-colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ecRNfaY-MTG"
      },
      "source": [
        "# 1. Clone Stable Diffusion Repo\n",
        "\n",
        "You can use your Google Drive storage as installation folder for Stable Diffusion.\n",
        "If you don't have any storage left on Google Drive you can leave `Use_Gdrive` unchecked, making it so will make all of Stable Diffusion files installed on Colab storage instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pbo8PKYSpavo",
        "outputId": "2adb5d1a-e553-489b-f400-580f35ea27fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done! Proceed to next step.\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "from google.colab import drive\n",
        "import os\n",
        "import time\n",
        "\n",
        "Use_Gdrive = False #@param {type:\"boolean\"}\n",
        "\n",
        "if Use_Gdrive == True:\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "if not os.path.exists(\"/content/gdrive/MyDrive/\"):\n",
        "  print(\"Google drive not connected, using Colab storage instead...\")\n",
        "  !mkdir -p /content/gdrive/MyDrive/\n",
        "\n",
        "time.sleep(3)\n",
        "%cd /content/gdrive/MyDrive\n",
        "!git clone --depth 1 -q --branch master https://github.com/AUTOMATIC1111/stable-diffusion-webui\n",
        "%cd /content/gdrive/MyDrive/stable-diffusion-webui\n",
        "!git reset --hard\n",
        "time.sleep(1)\n",
        "!rm webui.sh\n",
        "!git pull\n",
        "\n",
        "clear_output()\n",
        "print(\"Done! Proceed to next step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVFpAb0V-MTH"
      },
      "source": [
        "# 2. Installing Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBbcB4vwj_jm",
        "outputId": "6de0b2e5-5648-4a75-cfd2-2e265e2da916"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done! Proceed to next step.\n"
          ]
        }
      ],
      "source": [
        "!apt -y update -qq\n",
        "!wget http://launchpadlibrarian.net/367274644/libgoogle-perftools-dev_2.5-2.2ubuntu3_amd64.deb\n",
        "!wget http://launchpadlibrarian.net/367274642/google-perftools_2.5-2.2ubuntu3_all.deb\n",
        "!wget http://launchpadlibrarian.net/367274648/libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb\n",
        "!wget http://launchpadlibrarian.net/367274647/libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb\n",
        "!apt install -qq libunwind8-dev\n",
        "!dpkg -i *.deb\n",
        "%env LD_PRELOAD=libtcmalloc.so\n",
        "!rm *.deb\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "%pip install --upgrade fastapi==0.90.1\n",
        "%pip install huggingface-hub==0.11.1\n",
        "\n",
        "clear_output()\n",
        "print(\"Done! Proceed to next step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkfOkgGY-MTI"
      },
      "source": [
        "# 3. Model Download\n",
        "\n",
        "You can also install other model by filling `Model_Link` and `Model_Filename`. Example:\n",
        "* `Model_Link`: `https://civitai.com/api/download/models/8298`\n",
        "* `Model_Filename`: `cetusMix_cetusVersion2`\n",
        "\n",
        "Since the model type that we're going to download is `SafeTensor`, you need to check the `safetensors` checkbox."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CdEIDFD-MTI",
        "outputId": "56c0e0ea-0ac8-4e12-a6cd-51668a026ba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model downloaded, using the trained model...\n",
            "Done! Proceed to next step.\n"
          ]
        }
      ],
      "source": [
        "Model_Name = \"SD v1.5\" #@param [\"SD v1.5\", \"ChilloutMix\", \"Grap2.5\", \"Perfect World v2\", \"dalcefruit v4.1\", \"Counterfeit vefo_painting 3rd\", \"MIX-Pro-V3\", \"Meow Mix\"]\n",
        "\n",
        "#@markdown Or\n",
        "Model_Link = \"\" #@param {type:\"string\"}\n",
        "Model_Filename = \"\" #@param {type:\"string\"}\n",
        "safetensors = False #@param {type:\"boolean\"}\n",
        "\n",
        "pth = \"/content/gdrive/MyDrive/stable-diffusion-webui/models/Stable-diffusion\"\n",
        "ext_path = \"/content/gdrive/MyDrive/stable-diffusion-webui/extensions\"\n",
        "vae_path = \"/content/gdrive/MyDrive/stable-diffusion-webui/models/VAE\"\n",
        "lora_path = \"/content/gdrive/MyDrive/stable-diffusion-webui/models/Lora\"\n",
        "controlnet_path = \"/content/gdrive/MyDrive/stable-diffusion-webui/extensions/sd-webui-controlnet/models\"\n",
        "embd_path = \"/content/gdrive/MyDrive/stable-diffusion-webui/embeddings\"\n",
        "hypernet_path = \"/content/gdrive/MyDrive/stable-diffusion-webui/models/hypernetworks\"\n",
        "if Model_Link != \"\":\n",
        "  time.sleep(1)\n",
        "  %cd $pth\n",
        "  clear_output()\n",
        "  if not safetensors:\n",
        "    modelname = Model_Filename + \".ckpt\"\n",
        "  else:\n",
        "    modelname = Model_Filename + \".safetensors\"\n",
        "  time.sleep(1)\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M $Model_Link -d $pth -o $modelname\n",
        "  if os.path.exists(f'{pth}/{modelname}'):\n",
        "    clear_output()\n",
        "    print(\"Model downloaded, using the trained model...\")\n",
        "    print(\"Done! Proceed to next step.\")\n",
        "  else:\n",
        "    print(\"Invalid link, please check the link or perhaps you didn't write the filename?\")\n",
        "elif Model_Name == \"SD v1.5\":\n",
        "  time.sleep(1)\n",
        "  %cd $pth\n",
        "  clear_output()\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt -d $pth -o v1-5-pruned-emaonly.ckpt\n",
        "  if os.path.exists(f'{pth}/v1-5-pruned-emaonly.ckpt'):\n",
        "    clear_output()\n",
        "    print(\"Model downloaded, using the trained model...\")\n",
        "    print(\"Done! Proceed to next step.\")\n",
        "  else:\n",
        "    print(\"Something went wrong...\")\n",
        "elif Model_Name == \"ChilloutMix\":\n",
        "  time.sleep(1)\n",
        "  %cd $pth\n",
        "  clear_output()\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://civitai.com/api/download/models/11745 -d $pth -o chilloutmix_NiPrunedFp32Fix.safetensors\n",
        "  if os.path.exists(f'{pth}/chilloutmix_NiPrunedFp32Fix.safetensors'):\n",
        "    clear_output()\n",
        "    print(\"Model downloaded, using the trained model...\")\n",
        "    print(\"Done! Proceed to next step.\")\n",
        "  else:\n",
        "    print(\"Something went wrong...\")\n",
        "elif Model_Name == \"Grapefruit v4.1\":\n",
        "  time.sleep(1)\n",
        "  %cd $pth\n",
        "  clear_output()\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://civitai.com/api/download/models/16619 -d $pth -o grapefruitHentaiModel_grapefruitv41.safetensors\n",
        "  if os.path.exists(f'{pth}/grapefruitHentaiModel_grapefruitv41.safetensors'):\n",
        "    clear_output()\n",
        "    print(\"Model downloaded, using the trained model...\")\n",
        "    print(\"Done! Proceed to next step.\")\n",
        "  else:\n",
        "    print(\"Something went wrong...\")\n",
        "elif Model_Name == \"Counterfeit v2.5\":\n",
        "  time.sleep(1)\n",
        "  %cd $pth\n",
        "  clear_output()\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://civitai.com/api/download/models/7425 -d $pth -o CounterfeitV25_25.safetensors\n",
        "  if os.path.exists(f'{pth}/CounterfeitV25_25.safetensors'):\n",
        "    clear_output()\n",
        "    print(\"Model downloaded, using the trained model...\")\n",
        "    print(\"Done! Proceed to next step.\")\n",
        "  else:\n",
        "    print(\"Something went wrong...\")\n",
        "elif Model_Name == \"Perfect World v2\":\n",
        "  time.sleep(1)\n",
        "  %cd $pth\n",
        "  clear_output()\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://civitai.com/api/download/models/19084 -d $pth -o perfectWorld_v2Baked.safetensors\n",
        "  if os.path.exists(f'{pth}/perfectWorld_v2Baked.safetensors'):\n",
        "    clear_output()\n",
        "    print(\"Model downloaded, using the trained model...\")\n",
        "    print(\"Done! Proceed to next step.\")\n",
        "  else:\n",
        "    print(\"Something went wrong...\")\n",
        "elif Model_Name == \"dalcefo_painting 3rd\":\n",
        "  time.sleep(1)\n",
        "  %cd $pth\n",
        "  clear_output()\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://civitai.com/api/download/models/18446 -d $pth -o dalcefoPainting_3rd.safetensors\n",
        "  if os.path.exists(f'{pth}/dalcefoPainting_3rd.safetensors'):\n",
        "    clear_output()\n",
        "    print(\"Model downloaded, using the trained model...\")\n",
        "    print(\"Done! Proceed to next step.\")\n",
        "  else:\n",
        "    print(\"Something went wrong...\")\n",
        "elif Model_Name == \"MIX-Pro-V3\":\n",
        "  time.sleep(1)\n",
        "  %cd $pth\n",
        "  clear_output()\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://civitai.com/api/download/models/8511 -d $pth -o mixProV3_v3.safetensors\n",
        "  if os.path.exists(f'{pth}/mixProV3_v3.safetensors'):\n",
        "    clear_output()\n",
        "    print(\"Model downloaded, using the trained model...\")\n",
        "    print(\"Done! Proceed to next step.\")\n",
        "  else:\n",
        "    print(\"Something went wrong...\")\n",
        "elif Model_Name == \"Meow Mix\":\n",
        "  time.sleep(1)\n",
        "  %cd $pth\n",
        "  clear_output()\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://civitai.com/api/download/models/21758 -d $pth -o meowMixRealistic_prunedFp16FIXED.safetensors\n",
        "  if os.path.exists(f'{pth}/meowMixRealistic_prunedFp16FIXED.safetensors'):\n",
        "    clear_output()\n",
        "    print(\"Model downloaded, using the trained model...\")\n",
        "    print(\"Done! Proceed to next step.\")\n",
        "  else:\n",
        "    print(\"Something went wrong...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYfMk2eH-MTJ"
      },
      "source": [
        "# 4. Downloading Extensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htqprIAkrX1N",
        "outputId": "f5c8eb08-23d1-4ee2-cde4-5964db691db4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done! Proceed to next step.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/SlavyanDesu/sd-webui-tunnels $ext_path/sd-webui-tunnels\n",
        "!git clone https://github.com/DominikDoom/a1111-sd-webui-tagcomplete $ext_path/a1111-sd-webui-tagcomplete\n",
        "!git clone https://github.com/Coyote-A/ultimate-upscale-for-automatic1111.git $ext_path/ultimate-upscale-for-automatic1111\n",
        "!git clone https://github.com/AlUlkesh/stable-diffusion-webui-images-browser.git $ext_path/stable-diffusion-webui-images-browser\n",
        "!git clone https://github.com/camenduru/sd-civitai-browser $ext_path/sd-civitai-browser\n",
        "!git clone https://github.com/catppuccin/stable-diffusion-webui $ext_path/catppuccin-theme\n",
        "!git clone https://github.com/Mikubill/sd-webui-controlnet $ext_path/sd-webui-controlnet\n",
        "!git clone https://github.com/fkunn1326/openpose-editor $ext_path/openpose-editor\n",
        "!git clone https://github.com/KohakuBlueleaf/a1111-sd-webui-locon $ext_path/a1111-sd-webui-locon\n",
        "!git clone https://github.com/camenduru/stable-diffusion-webui-huggingface $ext_path/stable-diffusion-webui-huggingface\n",
        "\n",
        "clear_output()\n",
        "print(\"Done! Proceed to next step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWzWCvtF-MTK"
      },
      "source": [
        "# 5. Downloading ControlNet Models + Hypernetworks + Extra models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLBAXoB--MTK",
        "outputId": "26e32ea9-1b25-43ca-efd7-7acb8f035a6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done! Proceed to next step.\n"
          ]
        }
      ],
      "source": [
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_canny-fp16.safetensors -d $controlnet_path -o control_canny-fp16.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_depth-fp16.safetensors -d $controlnet_path -o control_depth-fp16.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_hed-fp16.safetensors -d $controlnet_path -o control_hed-fp16.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_mlsd-fp16.safetensors -d $controlnet_path -o control_mlsd-fp16.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_normal-fp16.safetensors -d $controlnet_path -o control_normal-fp16.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_openpose-fp16.safetensors -d $controlnet_path -o control_openpose-fp16.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_scribble-fp16.safetensors -d $controlnet_path -o control_scribble-fp16.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_seg-fp16.safetensors -d $controlnet_path -o control_seg-fp16.safetensors\n",
        "\n",
        "\n",
        "#Modified part here\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/eimiss/EimisAnimeDiffusion_1.0v/resolve/main/EimisAnimeDiffusion_1-0v.ckpt -d $pth -o \"EimisAnimeDiffusion_1.0v.ckpt\"\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://civitai.com/api/download/models/5255 -d $hypernet_path -o \"Toru8pWavenChibi_wavenchibiV10b.pt\"\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/anything-v3-full.safetensors -d $pth -o \"anything-v3-full.safetensors\"\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_nsfw.safetensors -d $pth -o \"AbyssOrangeMix2_nsfw.safetensors\"\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/swl-models/mix-pro-v3/resolve/main/mix-pro-v3-non-ema-fp16.safetensors -d $pth -o \"mix-pro-v3-non-ema-fp16.safetensors\"\n",
        "\n",
        "#End, I just put my fav models & hypernet and thats it\n",
        "clear_output()\n",
        "print(\"Done! Proceed to next step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff1A2lm_-MTL"
      },
      "source": [
        "# 6. Downloading VAE and Embeddings\n",
        "\n",
        "If you want to add other VAE please fill `VAE_Link` and `VAE_Filename`. Example:\n",
        "* `VAE_Link`: `https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt`\n",
        "* `VAE_Filename`: `vae-ft-ema-560000-ema-pruned.ckpt`\n",
        "\n",
        "For embedding:\n",
        "* `Embedding_Link`: `https://civitai.com/api/download/models/6197`\n",
        "* `Embedding_Filename`: `corneo_makima.pt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NdQZHVEsjRF",
        "outputId": "b5cbd8c9-dcf5-4ecf-8529-ea62dea50178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done! Proceed to next step.\n"
          ]
        }
      ],
      "source": [
        "#@markdown For VAE\n",
        "VAE_Link = \"\" #@param {type:\"string\"}\n",
        "VAE_Filename = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown For embedding\n",
        "Embedding_Link = \"\" #@param {type:\"string\"}\n",
        "Embedding_Filename = \"\" #@param {type:\"string\"}\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime.ckpt -d $vae_path -o kl-f8-anime.ckpt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt -d $vae_path -o kl-f8-anime2.ckpt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/gsdf/Counterfeit-V2.5/resolve/main/Counterfeit-V2.5.vae.pt -d $vae_path -o Counterfeit-V2.5.vae.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/VAEs/orangemix.vae.pt -d $vae_path -o orangemix.vae.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt -d $vae_path -o vae-ft-mse-840000-ema-pruned.ckpt\n",
        "\n",
        "import shutil\n",
        "shutil.rmtree(embd_path)\n",
        "%cd /content/gdrive/MyDrive/stable-diffusion-webui\n",
        "!git clone https://huggingface.co/SlavyanDesu/embeddings\n",
        "\n",
        "if VAE_Link != \"\":\n",
        "  time.sleep(1)\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M $VAE_Link -d $vae_path -o $VAE_Filename\n",
        "  if os.path.exists(f'{vae_path}/{VAE_Filename}'):\n",
        "    clear_output()\n",
        "    print(\"Custom VAE downloaded!\")\n",
        "    print(\"Done! Proceed to next step.\")\n",
        "  else:\n",
        "    print(\"Invalid link, please check the link...\")\n",
        "if Embedding_Link != \"\":\n",
        "  time.sleep(1)\n",
        "  !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M $Embedding_Link -d $embd_path -o $Embedding_Filename\n",
        "  if os.path.exists(f'{embd_path}/{Embedding_Filename}'):\n",
        "    clear_output()\n",
        "    print(\"Custom embedding downloaded!\")\n",
        "    print(\"Done! Proceed to next step.\")\n",
        "  else:\n",
        "    print(\"Invalid link, please check the link or perhaps you didn't write the filename?\")\n",
        "\n",
        "clear_output()\n",
        "print(\"Done! Proceed to next step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D62QC2Th-MTL"
      },
      "source": [
        "# 7. Downloading LORA(s)\n",
        "\n",
        "If you want to download other LORA, you can download it via CivitAi browser extension which is already installed on this Stable Diffusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzQUdwPu-MTL",
        "outputId": "a7664107-d0dc-4ea6-9eb2-dd35dc28c021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done! Proceed to next step.\n"
          ]
        }
      ],
      "source": [
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/SlavyanDesu/ChilloutMixLora/blob/main/koreanDollLikeness_v15.safetensors -d $lora_path -o koreanDollLikeness_v15.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/SlavyanDesu/ChilloutMixLora/blob/main/japaneseDollLikeness.safetensors -d $lora_path -o japaneseDollLikeness.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/SlavyanDesu/ChilloutMixLora/blob/main/taiwanDollLikeness.safetensors -d $lora_path -o taiwanDollLikeness.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://civitai.com/api/download/models/9942 -d $lora_path -o Helltaker_LoRA.safetensors\n",
        "clear_output()\n",
        "print(\"Done! Proceed to next step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3el_qwyZ-MTL"
      },
      "source": [
        "# 8. Running Stable Diffusion\n",
        "You can use any command line arguments, please visit [this](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Command-Line-Arguments-and-Settings#command-line-arguments) for more info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW1YZd_xteaK",
        "outputId": "5d8c349f-3a47-49fa-fcdb-51366cc1acb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.9.16 (main, Dec  7 2022, 01:11:51) \n",
            "[GCC 9.4.0]\n",
            "Commit hash: a9fed7c364061ae6efb37f797b6b522cb3cf7aa2\n",
            "Installing gfpgan\n",
            "Installing clip\n",
            "Installing open_clip\n",
            "Installing xformers\n",
            "Cloning Stable Diffusion into /content/gdrive/MyDrive/stable-diffusion-webui/repositories/stable-diffusion-stability-ai...\n",
            "Cloning Taming Transformers into /content/gdrive/MyDrive/stable-diffusion-webui/repositories/taming-transformers...\n",
            "Cloning K-diffusion into /content/gdrive/MyDrive/stable-diffusion-webui/repositories/k-diffusion...\n",
            "Cloning CodeFormer into /content/gdrive/MyDrive/stable-diffusion-webui/repositories/CodeFormer...\n",
            "Cloning BLIP into /content/gdrive/MyDrive/stable-diffusion-webui/repositories/BLIP...\n",
            "Installing requirements for CodeFormer\n",
            "Installing requirements for Web UI\n",
            "\n",
            "Installing pycloudflared\n",
            "\n",
            "\n",
            "Installing sd-webui-controlnet requirement: svglib\n",
            "\n",
            "Launching Web UI with arguments: --share --xformers --enable-insecure-extension-access --disable-safe-unpickle --remotemoe --no-half-vae\n",
            "2023-03-18 09:12:54.216787: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-18 09:12:57.725698: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-18 09:12:57.726003: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-18 09:12:57.726028: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "reading lora /content/gdrive/MyDrive/stable-diffusion-webui/models/Lora/japaneseDollLikeness.safetensors: AssertionError\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/extensions-builtin/Lora/lora.py\", line 53, in __init__\n",
            "    self.metadata = sd_models.read_metadata_from_safetensors(filename)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/sd_models.py\", line 221, in read_metadata_from_safetensors\n",
            "    assert metadata_len > 2 and json_start in (b'{\"', b\"{'\"), f\"{filename} is not a safetensors file\"\n",
            "AssertionError: /content/gdrive/MyDrive/stable-diffusion-webui/models/Lora/japaneseDollLikeness.safetensors is not a safetensors file\n",
            "\n",
            "reading lora /content/gdrive/MyDrive/stable-diffusion-webui/models/Lora/koreanDollLikeness_v15.safetensors: AssertionError\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/extensions-builtin/Lora/lora.py\", line 53, in __init__\n",
            "    self.metadata = sd_models.read_metadata_from_safetensors(filename)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/sd_models.py\", line 221, in read_metadata_from_safetensors\n",
            "    assert metadata_len > 2 and json_start in (b'{\"', b\"{'\"), f\"{filename} is not a safetensors file\"\n",
            "AssertionError: /content/gdrive/MyDrive/stable-diffusion-webui/models/Lora/koreanDollLikeness_v15.safetensors is not a safetensors file\n",
            "\n",
            "reading lora /content/gdrive/MyDrive/stable-diffusion-webui/models/Lora/taiwanDollLikeness.safetensors: AssertionError\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/extensions-builtin/Lora/lora.py\", line 53, in __init__\n",
            "    self.metadata = sd_models.read_metadata_from_safetensors(filename)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/sd_models.py\", line 221, in read_metadata_from_safetensors\n",
            "    assert metadata_len > 2 and json_start in (b'{\"', b\"{'\"), f\"{filename} is not a safetensors file\"\n",
            "AssertionError: /content/gdrive/MyDrive/stable-diffusion-webui/models/Lora/taiwanDollLikeness.safetensors is not a safetensors file\n",
            "\n",
            "Additional Network extension not installed, Only hijack built-in lora\n",
            "LoCon Extension hijack built-in lora successfully\n",
            "remote.moe detected, trying to connect...\n",
            "Warning: Permanently added 'remote.moe,159.69.126.209' (ECDSA) to the list of known hosts.\n",
            "Calculating sha256 for /content/gdrive/MyDrive/stable-diffusion-webui/models/Stable-diffusion/v1-5-pruned-emaonly.ckpt: cc6cb27103417325ff94f52b7a5d2dde45a7515b25c255d8e396c90014281516\n",
            "Loading weights [cc6cb27103] from /content/gdrive/MyDrive/stable-diffusion-webui/models/Stable-diffusion/v1-5-pruned-emaonly.ckpt\n",
            "Creating model from config: /content/gdrive/MyDrive/stable-diffusion-webui/configs/v1-inference.yaml\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "Downloading: 100% 961k/961k [00:00<00:00, 7.26MB/s]\n",
            "Downloading: 100% 525k/525k [00:00<00:00, 3.62MB/s]\n",
            "Downloading: 100% 389/389 [00:00<00:00, 351kB/s]\n",
            "Downloading: 100% 905/905 [00:00<00:00, 605kB/s]\n",
            "Downloading: 100% 4.52k/4.52k [00:00<00:00, 4.17MB/s]\n",
            "Applying xformers cross attention optimization.\n",
            "Textual inversion embeddings loaded(11): pureerosface_v1, EasyNegative, bad_prompt_version2, bad-artist-anime, ng_deepnegative_v1_75t, ulzzang-6500, bad-hands-5, charturnerv2, bad_prompt, bad-image-v2-39000, bad-artist\n",
            "Textual inversion embeddings skipped(3): nfixer, nrealfixer, nartfixer\n",
            "Model loaded in 49.0s (calculate hash: 28.3s, load weights from disk: 4.7s, create model: 3.0s, apply weights to model: 2.0s, apply half(): 1.1s, load VAE: 8.6s, move model to device: 1.0s, load textual inversion embeddings: 0.3s).\n",
            "Image Browser: Creating database\n",
            "Image Browser: Database created\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://d91209bd-cd6b-41c1.gradio.live\n",
            "Public WebUI Colab URL: http://zws2v35r5nfh27em6bdmnkqc4lhklsjkqfl6lsa3tl4vyvhm3wua.remote.moe\n",
            "Startup time: 100.4s (import gradio: 2.4s, import ldm: 8.0s, other imports: 3.6s, list extensions: 0.8s, setup codeformer: 0.2s, load scripts: 4.4s, load SD checkpoint: 49.5s, create ui: 21.5s, gradio launch: 10.0s).\n",
            "  0% 0/16 [00:00<?, ?it/s]\n",
            "  6% 1/16 [00:07<01:51,  7.41s/it]\n",
            " 19% 3/16 [00:07<00:23,  1.77s/it]\n",
            " 25% 4/16 [00:07<00:13,  1.13s/it]\n",
            " 31% 5/16 [00:07<00:08,  1.29it/s]\n",
            " 38% 6/16 [00:08<00:05,  1.77it/s]\n",
            " 44% 7/16 [00:08<00:03,  2.34it/s]\n",
            " 50% 8/16 [00:08<00:02,  2.95it/s]\n",
            " 56% 9/16 [00:08<00:01,  3.57it/s]\n",
            " 62% 10/16 [00:08<00:01,  4.18it/s]\n",
            " 69% 11/16 [00:08<00:01,  4.72it/s]\n",
            " 75% 12/16 [00:09<00:00,  5.19it/s]\n",
            " 81% 13/16 [00:09<00:00,  5.58it/s]\n",
            " 88% 14/16 [00:09<00:00,  5.87it/s]\n",
            " 94% 15/16 [00:09<00:00,  6.09it/s]\n",
            "100% 16/16 [00:09<00:00,  1.66it/s]\n",
            "\n",
            "Total progress: 100% 16/16 [00:06<00:00,  2.46it/s]\n",
            "Calculating sha256 for /content/gdrive/MyDrive/stable-diffusion-webui/models/Stable-diffusion/anything-v3-fp16-pruned.safetensors: 455c034d64a6bcef8921d17c656184fbc8296b6c1911e4e47ed53ea59828159b\n",
            "Loading weights [455c034d64] from /content/gdrive/MyDrive/stable-diffusion-webui/models/Stable-diffusion/anything-v3-fp16-pruned.safetensors\n",
            "changing setting sd_model_checkpoint to anything-v3-fp16-pruned.safetensors: RuntimeError\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/shared.py\", line 568, in set\n",
            "    self.data_labels[key].onchange()\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/call_queue.py\", line 15, in f\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/webui.py\", line 146, in <lambda>\n",
            "    shared.opts.onchange(\"sd_model_checkpoint\", wrap_queued_call(lambda: modules.sd_models.reload_model_weights()))\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/sd_models.py\", line 488, in reload_model_weights\n",
            "    state_dict = get_checkpoint_state_dict(checkpoint_info, timer)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/sd_models.py\", line 262, in get_checkpoint_state_dict\n",
            "    res = read_state_dict(checkpoint_info.filename)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/sd_models.py\", line 241, in read_state_dict\n",
            "    pl_sd = safetensors.torch.load_file(checkpoint_file, device=device)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/safetensors/torch.py\", line 101, in load_file\n",
            "    result[k] = f.get_tensor(k)\n",
            "RuntimeError: self.size(-1) must be divisible by 2 to view Byte as Half (different element sizes), but got 56350015\n",
            "\n",
            "Error completing request\n",
            "Arguments: ('task(i6vhn6ixpdpjaar)', 'a duck', '', [], 16, 0, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 512, 512, False, 0.7, 2, 'Latent', 0, 0, 0, [], 0, <scripts.external_code.ControlNetUnit object at 0x7f24e75b46d0>, False, False, 'positive', 'comma', 0, False, False, '', 1, '', 0, '', 0, '', True, False, False, False, 0, None, 50) {}\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/call_queue.py\", line 56, in f\n",
            "    res = list(func(*args, **kwargs))\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/call_queue.py\", line 37, in f\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/txt2img.py\", line 56, in txt2img\n",
            "    processed = process_images(p)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/processing.py\", line 486, in process_images\n",
            "    res = process_images_inner(p)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/processing.py\", line 625, in process_images_inner\n",
            "    uc = get_conds_with_caching(prompt_parser.get_learned_conditioning, negative_prompts, p.steps, cached_uc)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/processing.py\", line 570, in get_conds_with_caching\n",
            "    cache[1] = function(shared.sd_model, required_prompts, steps)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/prompt_parser.py\", line 140, in get_learned_conditioning\n",
            "    conds = model.get_learned_conditioning(texts)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/repositories/stable-diffusion-stability-ai/ldm/models/diffusion/ddpm.py\", line 665, in get_learned_conditioning\n",
            "    c = self.cond_stage_model.encode(c)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/repositories/stable-diffusion-stability-ai/ldm/modules/encoders/modules.py\", line 131, in encode\n",
            "    return self(text)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/repositories/stable-diffusion-stability-ai/ldm/modules/encoders/modules.py\", line 121, in forward\n",
            "    outputs = self.transformer(input_ids=tokens, output_hidden_states=self.layer==\"hidden\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/clip/modeling_clip.py\", line 811, in forward\n",
            "    return self.text_model(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/clip/modeling_clip.py\", line 708, in forward\n",
            "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/clip/modeling_clip.py\", line 223, in forward\n",
            "    inputs_embeds = self.token_embedding(input_ids)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/sparse.py\", line 160, in forward\n",
            "    return F.embedding(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\", line 2210, in embedding\n",
            "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)\n",
            "\n",
            "Error completing request\n",
            "Arguments: ('task(gnwuauem3up855o)', 'a duck', '', [], 16, 0, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 512, 512, False, 0.7, 2, 'Latent', 0, 0, 0, [], 0, <scripts.external_code.ControlNetUnit object at 0x7f24e75b46d0>, False, False, 'positive', 'comma', 0, False, False, '', 1, '', 0, '', 0, '', True, False, False, False, 0, None, 50) {}\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/call_queue.py\", line 56, in f\n",
            "    res = list(func(*args, **kwargs))\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/call_queue.py\", line 37, in f\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/txt2img.py\", line 56, in txt2img\n",
            "    processed = process_images(p)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/processing.py\", line 486, in process_images\n",
            "    res = process_images_inner(p)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/processing.py\", line 625, in process_images_inner\n",
            "    uc = get_conds_with_caching(prompt_parser.get_learned_conditioning, negative_prompts, p.steps, cached_uc)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/processing.py\", line 570, in get_conds_with_caching\n",
            "    cache[1] = function(shared.sd_model, required_prompts, steps)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/prompt_parser.py\", line 140, in get_learned_conditioning\n",
            "    conds = model.get_learned_conditioning(texts)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/repositories/stable-diffusion-stability-ai/ldm/models/diffusion/ddpm.py\", line 665, in get_learned_conditioning\n",
            "    c = self.cond_stage_model.encode(c)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/repositories/stable-diffusion-stability-ai/ldm/modules/encoders/modules.py\", line 131, in encode\n",
            "    return self(text)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/repositories/stable-diffusion-stability-ai/ldm/modules/encoders/modules.py\", line 121, in forward\n",
            "    outputs = self.transformer(input_ids=tokens, output_hidden_states=self.layer==\"hidden\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/clip/modeling_clip.py\", line 811, in forward\n",
            "    return self.text_model(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/clip/modeling_clip.py\", line 708, in forward\n",
            "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/clip/modeling_clip.py\", line 223, in forward\n",
            "    inputs_embeds = self.token_embedding(input_ids)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/sparse.py\", line 160, in forward\n",
            "    return F.embedding(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\", line 2210, in embedding\n",
            "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)\n",
            "\n",
            "Error completing request\n",
            "Arguments: ('task(83hejwkqzxxn89x)', 'a duck', '', [], 16, 0, False, False, 1, 1, 7, -1.0, -1.0, 0, 0, 0, False, 512, 512, False, 0.7, 2, 'Latent', 0, 0, 0, [], 0, <scripts.external_code.ControlNetUnit object at 0x7f24e75b46d0>, False, False, 'positive', 'comma', 0, False, False, '', 1, '', 0, '', 0, '', True, False, False, False, 0, None, 50) {}\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/call_queue.py\", line 56, in f\n",
            "    res = list(func(*args, **kwargs))\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/call_queue.py\", line 37, in f\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/txt2img.py\", line 56, in txt2img\n",
            "    processed = process_images(p)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/processing.py\", line 486, in process_images\n",
            "    res = process_images_inner(p)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/processing.py\", line 625, in process_images_inner\n",
            "    uc = get_conds_with_caching(prompt_parser.get_learned_conditioning, negative_prompts, p.steps, cached_uc)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/processing.py\", line 570, in get_conds_with_caching\n",
            "    cache[1] = function(shared.sd_model, required_prompts, steps)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/modules/prompt_parser.py\", line 140, in get_learned_conditioning\n",
            "    conds = model.get_learned_conditioning(texts)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/repositories/stable-diffusion-stability-ai/ldm/models/diffusion/ddpm.py\", line 665, in get_learned_conditioning\n",
            "    c = self.cond_stage_model.encode(c)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/repositories/stable-diffusion-stability-ai/ldm/modules/encoders/modules.py\", line 131, in encode\n",
            "    return self(text)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/gdrive/MyDrive/stable-diffusion-webui/repositories/stable-diffusion-stability-ai/ldm/modules/encoders/modules.py\", line 121, in forward\n",
            "    outputs = self.transformer(input_ids=tokens, output_hidden_states=self.layer==\"hidden\")\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/clip/modeling_clip.py\", line 811, in forward\n",
            "    return self.text_model(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/clip/modeling_clip.py\", line 708, in forward\n",
            "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/clip/modeling_clip.py\", line 223, in forward\n",
            "    inputs_embeds = self.token_embedding(input_ids)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/sparse.py\", line 160, in forward\n",
            "    return F.embedding(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\", line 2210, in embedding\n",
            "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
            "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)\n",
            "\n",
            "Interrupted with signal 2 in <frame at 0xf256710, file '/content/gdrive/MyDrive/stable-diffusion-webui/webui.py', line 206, code wait_on_server>\n"
          ]
        }
      ],
      "source": [
        "!sed -i -e 's/checkout {commithash}/checkout --force {commithash}/g' /content/gdrive/MyDrive/stable-diffusion-webui/launch.py\n",
        "!COMMANDLINE_ARGS=\"--share --xformers --enable-insecure-extension-access --reinstall-xformers --disable-safe-unpickle --remotemoe --no-half-vae\" REQS_FILE=\"requirements.txt\" python launch.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
